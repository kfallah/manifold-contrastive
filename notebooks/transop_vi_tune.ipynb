{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate needed for notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = \"../results/Transop_sep-loss/\"\n",
    "current_checkpoint = 400\n",
    "device_idx = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /storage/home/hcoda1/0/kfallah3/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/storage/home/hcoda1/0/kfallah3/.conda/envs/manifold-contrastive/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/storage/home/hcoda1/0/kfallah3/.conda/envs/manifold-contrastive/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/storage/home/hcoda1/0/kfallah3/.conda/envs/manifold-contrastive/lib/python3.9/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import math\n",
    "sys.path.append(os.path.dirname(os.getcwd()) + \"/src/\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import omegaconf\n",
    "\n",
    "from eval.utils import encode_features\n",
    "from model.model import Model\n",
    "from model.config import ModelConfig\n",
    "from experiment import ExperimentConfig\n",
    "from dataloader.contrastive_dataloader import get_dataloader\n",
    "from dataloader.utils import get_unaugmented_dataloader\n",
    "\n",
    "# Set the default device\n",
    "default_device = torch.device(\"cuda:0\")\n",
    "# Load config\n",
    "cfg = omegaconf.OmegaConf.load(run_dir + \".hydra/config.yaml\")\n",
    "cfg.model_cfg.backbone_cfg.load_backbone = None\n",
    "\n",
    "# Load model\n",
    "default_model_cfg = ModelConfig()\n",
    "model = Model.initialize_model(cfg.model_cfg, cfg.train_dataloader_cfg.dataset_cfg.dataset_name, device_idx)\n",
    "state_dict = torch.load(run_dir + f\"checkpoints/checkpoint_epoch{current_checkpoint}.pt\", map_location=default_device)\n",
    "model.load_state_dict(state_dict['model_state'])\n",
    "# Manually override directory for dataloaders\n",
    "cfg.train_dataloader_cfg.dataset_cfg.dataset_dir = \"../datasets\"\n",
    "cfg.train_dataloader_cfg.batch_size = 128\n",
    "# Load dataloaders\n",
    "train_dataset, train_dataloader = get_dataloader(cfg.train_dataloader_cfg)\n",
    "\n",
    "# Load transport operators\n",
    "psi = model.contrastive_header.transop_header.transop.get_psi()\n",
    "backbone = model.backbone\n",
    "nn_bank = model.contrastive_header.transop_header.nn_memory_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    num_epochs: int = 300\n",
    "\n",
    "    kl_weight = 1.0e-3\n",
    "    enable_c_l2: bool = False\n",
    "    c_l2_weight: float = 1.0e-3\n",
    "\n",
    "@dataclass\n",
    "class CoeffEncoderConfig:\n",
    "    variational: bool = True\n",
    "\n",
    "    feature_dim: float = 512\n",
    "    hidden_dim: float = 2048\n",
    "    scale_prior: float = 0.02\n",
    "    threshold: float = 0.032\n",
    "    \n",
    "    enable_c_l2: bool = False\n",
    "    c_l2_weight: float = 1.0e-3\n",
    "    \n",
    "    lr: float = 0.03\n",
    "    weight_decay: float = 1.0e-5\n",
    "\n",
    "class VIEncoder(nn.Module):\n",
    "    def __init__(self, cfg: CoeffEncoderConfig):\n",
    "        super(VIEncoder, self).__init__()\n",
    "        self.feat_extract = nn.Sequential(\n",
    "            nn.Linear(128, cfg.hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(cfg.hidden_dim, cfg.hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(cfg.hidden_dim, cfg.feature_dim),\n",
    "        )\n",
    "\n",
    "        self.cfg = cfg\n",
    "        if cfg.variational:\n",
    "            self.scale = nn.Linear(cfg.feature_dim, 100)\n",
    "            self.shift = nn.Linear(cfg.feature_dim, 100)\n",
    "        else:\n",
    "            self.pred = nn.Linear(cfg.feature_dim, 100)\n",
    "\n",
    "    def soft_threshold(self, z: torch.Tensor, lambda_: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.relu(torch.abs(z) - lambda_) * torch.sign(z)\n",
    "\n",
    "    def kl_loss(self, log_scale, shift):\n",
    "        prior_scale = torch.ones_like(log_scale) * self.cfg.scale_prior\n",
    "        prior_log_scale = torch.log(prior_scale)\n",
    "        scale = torch.exp(log_scale)\n",
    "        laplace_kl = ((shift).abs() / prior_scale) + prior_log_scale - log_scale - 1\n",
    "        laplace_kl += (scale / prior_scale) * (-((shift).abs() / scale)).exp()\n",
    "        return laplace_kl.sum(dim=-1).mean()\n",
    "\n",
    "    def reparameterize(self, log_scale, shift, psi):\n",
    "        # Reparameterize\n",
    "        noise = torch.rand_like(log_scale) - 0.5\n",
    "        scale = torch.exp(log_scale)\n",
    "        eps = -scale * torch.sign(noise) * torch.log((1.0 - 2.0 * torch.abs(noise)).clamp(min=1e-6, max=1e6))\n",
    "        c = shift + eps\n",
    "\n",
    "        # Threshold\n",
    "        c_thresh = self.soft_threshold(eps.detach(), self.cfg.threshold)\n",
    "        non_zero = torch.nonzero(c_thresh, as_tuple=True)\n",
    "        c_thresh[non_zero] = shift[non_zero].detach() + c_thresh[non_zero]\n",
    "        c = c + c_thresh - c.detach()\n",
    "        return c\n",
    "\n",
    "    def forward(self, x0, x1, psi):\n",
    "        z = self.feat_extract(torch.cat((x0, x1), dim=-1))\n",
    "        if not self.cfg.variational:\n",
    "            return self.pred(z), torch.tensor(0.)\n",
    "\n",
    "        log_scale, shift = self.scale(z), self.shift(z)\n",
    "        log_scale += torch.log(torch.ones_like(log_scale) * self.cfg.scale_prior)\n",
    "\n",
    "        # Reparameterization\n",
    "        c = self.reparameterize(log_scale, shift, psi)\n",
    "\n",
    "        return c, self.kl_loss(log_scale, shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.public.linear_warmup_cos_anneal import LinearWarmupCosineAnnealingLR\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_dataset, train_dataloader = get_dataloader(cfg.train_dataloader_cfg)\n",
    "\n",
    "vi_cfg = CoeffEncoderConfig()\n",
    "encoder = VIEncoder(vi_cfg).cuda()\n",
    "opt = torch.optim.SGD(encoder.parameters(), lr=vi_cfg.lr, nesterov=True, momentum=0.9, weight_decay=vi_cfg.weight_decay)\n",
    "iters_per_epoch = len(train_dataloader)\n",
    "scheduler = LinearWarmupCosineAnnealingLR(opt, warmup_epochs=10 * iters_per_epoch, max_epochs=iters_per_epoch*300, eta_min=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 -- TO loss: 1.447E-01, KL loss: 5.691E+01, dist improve: 1.084E+00, c nonzero: 20.200, c mag: 0.031\n",
      "Iter 500 -- TO loss: 1.064E-01, KL loss: 1.041E+01, dist improve: 1.083E+00, c nonzero: 20.159, c mag: 0.022\n",
      "Iter 1000 -- TO loss: 7.846E-02, KL loss: 1.367E+00, dist improve: 1.114E+00, c nonzero: 20.100, c mag: 0.020\n",
      "Iter 1500 -- TO loss: 7.699E-02, KL loss: 4.222E-01, dist improve: 1.113E+00, c nonzero: 19.892, c mag: 0.020\n",
      "Iter 2000 -- TO loss: 7.622E-02, KL loss: 3.197E-01, dist improve: 1.109E+00, c nonzero: 19.685, c mag: 0.020\n",
      "Iter 2500 -- TO loss: 7.562E-02, KL loss: 3.430E-01, dist improve: 1.107E+00, c nonzero: 19.567, c mag: 0.020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     z0, z1 \u001b[39m=\u001b[39m backbone(x0), backbone(x1)\n\u001b[0;32m---> 14\u001b[0m z1 \u001b[39m=\u001b[39m nn_bank(z1\u001b[39m.\u001b[39;49mdetach(), update\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     16\u001b[0m z0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(torch\u001b[39m.\u001b[39msplit(z0, \u001b[39m64\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[1;32m     17\u001b[0m z1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(torch\u001b[39m.\u001b[39msplit(z1, \u001b[39m64\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m64\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/manifold-contrastive/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/manifold-contrastive/lib/python3.9/site-packages/lightly/models/modules/nn_memory_bank.py:53\u001b[0m, in \u001b[0;36mNNMemoryBankModule.forward\u001b[0;34m(self, output, update)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     42\u001b[0m             output: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m     43\u001b[0m             update: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     \u001b[39m\"\"\"Returns nearest neighbour of output tensor from memory bank\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     output, bank \u001b[39m=\u001b[39m \\\n\u001b[0;32m---> 53\u001b[0m         \u001b[39msuper\u001b[39;49m(NNMemoryBankModule, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mforward(output, update\u001b[39m=\u001b[39;49mupdate)\n\u001b[1;32m     54\u001b[0m     bank \u001b[39m=\u001b[39m bank\u001b[39m.\u001b[39mto(output\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mt()\n\u001b[1;32m     56\u001b[0m     output_normed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnormalize(output, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/manifold-contrastive/lib/python3.9/site-packages/lightly/loss/memory_bank.py:122\u001b[0m, in \u001b[0;36mMemoryBankModule.forward\u001b[0;34m(self, output, labels, update)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39m# only update memory bank if we later do backward pass (gradient)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m update:\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dequeue_and_enqueue(output)\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m output, bank\n",
      "File \u001b[0;32m~/.conda/envs/manifold-contrastive/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/manifold-contrastive/lib/python3.9/site-packages/lightly/loss/memory_bank.py:86\u001b[0m, in \u001b[0;36mMemoryBankModule._dequeue_and_enqueue\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbank_ptr[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbank[:, ptr:ptr \u001b[39m+\u001b[39m batch_size] \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     87\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbank_ptr[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m ptr \u001b[39m+\u001b[39m batch_size\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transop_loss_save = []\n",
    "kl_loss_save = []\n",
    "dw_loss_save = []\n",
    "c_save = []\n",
    "\n",
    "for i in range(300):\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        curr_iter = i*len(train_dataloader) + idx\n",
    "        x0, x1 = batch[0][0], batch[0][1]\n",
    "        x0, x1 = x0.cuda(), x1.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z0, z1 = backbone(x0), backbone(x1)\n",
    "        z1 = nn_bank(z1.detach(), update=True).detach()\n",
    "\n",
    "        z0 = torch.stack(torch.split(z0, 64, dim=-1)).transpose(0, 1).reshape(-1, 64)\n",
    "        z1 = torch.stack(torch.split(z1, 64, dim=-1)).transpose(0, 1).reshape(-1, 64)\n",
    "\n",
    "        c, kl_loss = encoder(z0, z1, psi)\n",
    "        T = torch.matrix_exp(torch.einsum(\"bm,mpk->bpk\", c, psi))\n",
    "        z1_hat = (T @ z0.unsqueeze(-1)).squeeze(-1)\n",
    "        transop_loss = torch.nn.functional.mse_loss(z1_hat, z1, reduction=\"none\")\n",
    "\n",
    "        loss = transop_loss.mean() + 8.0e-3*kl_loss\n",
    "        if vi_cfg.enable_c_l2:\n",
    "            l2_reg = (c**2).sum(dim=-1).mean()\n",
    "            loss += vi_cfg.c_l2_weight * l2_reg\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        transop_loss_save.append(transop_loss.mean().item())\n",
    "        kl_loss_save.append(kl_loss.item())\n",
    "        dw_bw_points = torch.nn.functional.mse_loss(z0, z1, reduction=\"none\").mean(dim=-1)\n",
    "        dw_loss_save.append((transop_loss.mean(dim=-1) / dw_bw_points).mean().item())\n",
    "        c_save.append(c.detach().cpu())\n",
    "\n",
    "        if curr_iter % 500 == 0:\n",
    "            last_c = torch.cat(c_save[-500:])\n",
    "            c_nz = torch.count_nonzero(last_c, dim=-1).float().mean()\n",
    "            c_mag = last_c[last_c.abs() > 0].abs().mean()\n",
    "            print(f\"Iter {curr_iter} -- TO loss: {np.mean(transop_loss_save[-500:]):.3E},\" +\n",
    "                  f\" KL loss: {np.mean(kl_loss_save[-500:]):.3E},\" +\n",
    "                  f\" dist improve: {np.mean(dw_loss_save[-500:]):.3E},\" +\n",
    "                  f\" c nonzero: {c_nz:.3f},\" +\n",
    "                  f\" c mag: {c_mag:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.manifold.l1_inference import infer_coefficients\n",
    "\n",
    "(loss, _, k), c = infer_coefficients(\n",
    "    z0[:128], z1[:128], psi, 0.05, max_iter=1200, tol=1e-5, num_trials=50, device=\"cuda:0\", lr=1e-2, decay=0.99, c_init=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810\n",
      "tensor(61.1953, device='cuda:0')\n",
      "tensor(0.0561, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "T = torch.matrix_exp(torch.einsum(\"bm,mpk->bpk\", c, psi))\n",
    "z1_hat = (T @ z0[:128].unsqueeze(-1)).squeeze(-1)\n",
    "transop_loss = torch.nn.functional.mse_loss(z1_hat, z1[:128], reduction=\"none\").mean(dim=-1)\n",
    "dw_bw_points = torch.nn.functional.mse_loss(z0[:128], z1[:128], reduction=\"none\").mean(dim=-1)\n",
    "\n",
    "print(k)\n",
    "print(torch.count_nonzero(c, dim=-1).float().mean())\n",
    "print((transop_loss / dw_bw_points).mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifold-contrastive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "daf518e7103f899566b3dc2466ae9f0356aaf5fcbd686287cae1870fd9416b4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
