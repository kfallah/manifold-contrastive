{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kfallah/anaconda3/envs/simclr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms.transforms as T\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()) + \"/src/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k)\n",
    "        return res\n",
    "\n",
    "def get_lr(step, total_steps, lr_max, lr_min):\n",
    "    \"\"\"Compute learning rate according to cosine annealing schedule.\"\"\"\n",
    "    return lr_min + (lr_max - lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kfallah/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/kfallah/anaconda3/envs/simclr/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kfallah/anaconda3/envs/simclr/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "weights = torch.load(\"../results/simclr_stl10_03-20-2023_16-16-57/checkpoints/checkpoint_epoch999.pt\")[\"model_state\"]\n",
    "#weights = torch.load(\"../results/vi-20s-40refine-thresh0.1-infonce1e-2_02-24-2023_12-41-03/checkpoints/checkpoint_epoch599.pt\")[\"model_state\"]\n",
    "#weights = torch.load(\"../results/vi-20s-infonce1e-2-constspeed-kl1e-4-attn_02-28-2023_10-41-44/checkpoints/checkpoint_epoch999.pt\")[\"model_state\"]\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet18\", pretrained=False).to(device)\n",
    "model.fc = nn.Identity()\n",
    "#model.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "#model.maxpool = nn.Identity()\n",
    "model.requires_grad = False\n",
    "\n",
    "own_state = model.state_dict()\n",
    "for name, param in weights.items():\n",
    "    name = name.replace(\"backbone.backbone_network.\", \"\")\n",
    "    if name not in own_state:\n",
    "        continue\n",
    "    if isinstance(param, nn.Parameter):\n",
    "        # backwards compatibility for serialized parameters\n",
    "        param = param.data\n",
    "    own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kfallah/anaconda3/envs/simclr/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t = T.Compose(\n",
    "    [T.Resize(70, interpolation=3), T.CenterCrop(64), T.ToTensor(), T.Normalize((0.43, 0.42, 0.39), (0.27, 0.26, 0.27))]\n",
    ")\n",
    "train_data = torchvision.datasets.STL10(\"../datasets\", split=\"train\", transform=t)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=512, drop_last=False)\n",
    "\n",
    "train_data = torchvision.datasets.STL10(\"../datasets\", split=\"test\", transform=t)\n",
    "val_dataloader = torch.utils.data.DataLoader(train_data, batch_size=512, shuffle=False)\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "for idx, batch in enumerate(train_dataloader):\n",
    "    x, y = batch\n",
    "    x = x.to(device)\n",
    "    feat = model(x)\n",
    "    train_x.append(feat.detach().cpu())\n",
    "    train_y.append(y)\n",
    "train_x = torch.cat(train_x)\n",
    "train_y = torch.cat(train_y)\n",
    "\n",
    "val_x = []\n",
    "val_y = []\n",
    "for idx, batch in enumerate(val_dataloader):\n",
    "    x, y = batch\n",
    "    x = x.to(device)\n",
    "    feat = model(x)\n",
    "    val_x.append(feat.detach().cpu())\n",
    "    val_y.append(y)\n",
    "val_x = torch.cat(val_x)\n",
    "val_y = torch.cat(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 0.7738750576972961\n",
      "Epoch 199: 0.7728750109672546\n",
      "Epoch 299: 0.7715000510215759\n",
      "Epoch 399: 0.7710000276565552\n",
      "Epoch 499: 0.7710000276565552\n"
     ]
    }
   ],
   "source": [
    "clf = nn.Linear(512, 10).to(device)\n",
    "\n",
    "lr_start, lr_end = 1e-2, 1e-6\n",
    "gamma = (lr_end / lr_start) ** (1 / 500)\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=lr_start, weight_decay=5e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "\n",
    "for e in range(500):\n",
    "    perm = torch.randperm(len(train_x)).view(-1, 500)\n",
    "    for idx in perm:\n",
    "        optimizer.zero_grad()\n",
    "        criterion(clf(train_x[idx]), train_y[idx]).backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    if (e+1) % 100 == 0:\n",
    "        y_pred = clf(val_x)\n",
    "        pred_top = y_pred.topk(max([1, 5]), 1, largest=True, sorted=True).indices\n",
    "        acc = {\n",
    "            t: (pred_top[:, :t] == val_y[..., None]).float().sum(1).mean().cpu().item()\n",
    "            for t in [1, 5]\n",
    "        }\n",
    "        print(f\"Epoch {e}: \" + str(acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 4.331E-01, val loss: 4.098E-01, top1 acc: 87.41%\n",
      "Epoch 2, train loss: 4.073E-01, val loss: 3.950E-01, top1 acc: 87.57%\n",
      "Epoch 3, train loss: 3.936E-01, val loss: 3.848E-01, top1 acc: 87.80%\n",
      "Epoch 4, train loss: 3.812E-01, val loss: 3.730E-01, top1 acc: 87.89%\n",
      "Epoch 5, train loss: 3.797E-01, val loss: 3.722E-01, top1 acc: 87.95%\n",
      "Epoch 6, train loss: 3.764E-01, val loss: 3.660E-01, top1 acc: 88.14%\n",
      "Epoch 7, train loss: 3.687E-01, val loss: 3.677E-01, top1 acc: 88.02%\n",
      "Epoch 8, train loss: 3.646E-01, val loss: 3.614E-01, top1 acc: 88.19%\n",
      "Epoch 9, train loss: 3.660E-01, val loss: 3.604E-01, top1 acc: 88.34%\n",
      "Epoch 10, train loss: 3.628E-01, val loss: 3.523E-01, top1 acc: 88.43%\n",
      "Epoch 11, train loss: 3.633E-01, val loss: 3.552E-01, top1 acc: 88.36%\n",
      "Epoch 12, train loss: 3.621E-01, val loss: 3.511E-01, top1 acc: 88.52%\n",
      "Epoch 13, train loss: 3.560E-01, val loss: 3.541E-01, top1 acc: 88.44%\n",
      "Epoch 14, train loss: 3.546E-01, val loss: 3.456E-01, top1 acc: 88.59%\n",
      "Epoch 15, train loss: 3.544E-01, val loss: 3.489E-01, top1 acc: 88.36%\n",
      "Epoch 16, train loss: 3.540E-01, val loss: 3.505E-01, top1 acc: 88.19%\n",
      "Epoch 17, train loss: 3.540E-01, val loss: 3.446E-01, top1 acc: 88.65%\n",
      "Epoch 18, train loss: 3.543E-01, val loss: 3.495E-01, top1 acc: 88.37%\n",
      "Epoch 19, train loss: 3.487E-01, val loss: 3.459E-01, top1 acc: 88.43%\n",
      "Epoch 20, train loss: 3.521E-01, val loss: 3.469E-01, top1 acc: 88.51%\n",
      "Epoch 21, train loss: 3.517E-01, val loss: 3.431E-01, top1 acc: 88.49%\n",
      "Epoch 22, train loss: 3.489E-01, val loss: 3.453E-01, top1 acc: 88.51%\n",
      "Epoch 23, train loss: 3.474E-01, val loss: 3.435E-01, top1 acc: 88.55%\n",
      "Epoch 24, train loss: 3.458E-01, val loss: 3.454E-01, top1 acc: 88.45%\n",
      "Epoch 25, train loss: 3.513E-01, val loss: 3.417E-01, top1 acc: 88.57%\n",
      "Epoch 26, train loss: 3.436E-01, val loss: 3.417E-01, top1 acc: 88.68%\n",
      "Epoch 27, train loss: 3.483E-01, val loss: 3.457E-01, top1 acc: 88.33%\n",
      "Epoch 28, train loss: 3.466E-01, val loss: 3.377E-01, top1 acc: 88.71%\n",
      "Epoch 29, train loss: 3.477E-01, val loss: 3.408E-01, top1 acc: 88.60%\n",
      "Epoch 30, train loss: 3.418E-01, val loss: 3.408E-01, top1 acc: 88.57%\n",
      "Epoch 31, train loss: 3.463E-01, val loss: 3.420E-01, top1 acc: 88.55%\n",
      "Epoch 32, train loss: 3.422E-01, val loss: 3.395E-01, top1 acc: 88.52%\n",
      "Epoch 33, train loss: 3.449E-01, val loss: 3.371E-01, top1 acc: 88.61%\n",
      "Epoch 34, train loss: 3.420E-01, val loss: 3.407E-01, top1 acc: 88.67%\n",
      "Epoch 35, train loss: 3.457E-01, val loss: 3.387E-01, top1 acc: 88.60%\n",
      "Epoch 36, train loss: 3.485E-01, val loss: 3.397E-01, top1 acc: 88.49%\n",
      "Epoch 37, train loss: 3.462E-01, val loss: 3.418E-01, top1 acc: 88.46%\n",
      "Epoch 38, train loss: 3.429E-01, val loss: 3.403E-01, top1 acc: 88.73%\n",
      "Epoch 39, train loss: 3.448E-01, val loss: 3.399E-01, top1 acc: 88.51%\n",
      "Epoch 40, train loss: 3.377E-01, val loss: 3.378E-01, top1 acc: 88.70%\n",
      "Epoch 41, train loss: 3.423E-01, val loss: 3.384E-01, top1 acc: 88.68%\n",
      "Epoch 42, train loss: 3.455E-01, val loss: 3.372E-01, top1 acc: 88.72%\n",
      "Epoch 43, train loss: 3.436E-01, val loss: 3.371E-01, top1 acc: 88.63%\n",
      "Epoch 44, train loss: 3.432E-01, val loss: 3.390E-01, top1 acc: 88.62%\n",
      "Epoch 45, train loss: 3.430E-01, val loss: 3.385E-01, top1 acc: 88.75%\n",
      "Epoch 46, train loss: 3.385E-01, val loss: 3.399E-01, top1 acc: 88.61%\n",
      "Epoch 47, train loss: 3.418E-01, val loss: 3.400E-01, top1 acc: 88.60%\n",
      "Epoch 48, train loss: 3.421E-01, val loss: 3.342E-01, top1 acc: 88.66%\n",
      "Epoch 49, train loss: 3.378E-01, val loss: 3.406E-01, top1 acc: 88.54%\n",
      "Epoch 50, train loss: 3.413E-01, val loss: 3.397E-01, top1 acc: 88.58%\n",
      "Epoch 51, train loss: 3.375E-01, val loss: 3.369E-01, top1 acc: 88.77%\n",
      "Epoch 52, train loss: 3.388E-01, val loss: 3.406E-01, top1 acc: 88.56%\n",
      "Epoch 53, train loss: 3.427E-01, val loss: 3.402E-01, top1 acc: 88.47%\n",
      "Epoch 54, train loss: 3.375E-01, val loss: 3.345E-01, top1 acc: 88.83%\n",
      "Epoch 55, train loss: 3.429E-01, val loss: 3.383E-01, top1 acc: 88.60%\n",
      "Epoch 56, train loss: 3.384E-01, val loss: 3.356E-01, top1 acc: 88.63%\n",
      "Epoch 57, train loss: 3.370E-01, val loss: 3.387E-01, top1 acc: 88.73%\n",
      "Epoch 58, train loss: 3.405E-01, val loss: 3.343E-01, top1 acc: 88.76%\n",
      "Epoch 59, train loss: 3.375E-01, val loss: 3.391E-01, top1 acc: 88.66%\n",
      "Epoch 60, train loss: 3.402E-01, val loss: 3.337E-01, top1 acc: 88.80%\n",
      "Epoch 61, train loss: 3.405E-01, val loss: 3.347E-01, top1 acc: 88.71%\n",
      "Epoch 62, train loss: 3.359E-01, val loss: 3.375E-01, top1 acc: 88.61%\n",
      "Epoch 63, train loss: 3.412E-01, val loss: 3.375E-01, top1 acc: 88.71%\n",
      "Epoch 64, train loss: 3.392E-01, val loss: 3.322E-01, top1 acc: 88.87%\n",
      "Epoch 65, train loss: 3.413E-01, val loss: 3.354E-01, top1 acc: 88.55%\n",
      "Epoch 66, train loss: 3.380E-01, val loss: 3.314E-01, top1 acc: 88.95%\n",
      "Epoch 67, train loss: 3.414E-01, val loss: 3.361E-01, top1 acc: 88.68%\n",
      "Epoch 68, train loss: 3.346E-01, val loss: 3.369E-01, top1 acc: 88.53%\n",
      "Epoch 69, train loss: 3.352E-01, val loss: 3.339E-01, top1 acc: 88.72%\n",
      "Epoch 70, train loss: 3.361E-01, val loss: 3.343E-01, top1 acc: 88.68%\n",
      "Epoch 71, train loss: 3.374E-01, val loss: 3.346E-01, top1 acc: 88.77%\n",
      "Epoch 72, train loss: 3.415E-01, val loss: 3.390E-01, top1 acc: 88.59%\n",
      "Epoch 73, train loss: 3.376E-01, val loss: 3.344E-01, top1 acc: 88.68%\n",
      "Epoch 74, train loss: 3.340E-01, val loss: 3.334E-01, top1 acc: 88.56%\n",
      "Epoch 75, train loss: 3.381E-01, val loss: 3.396E-01, top1 acc: 88.59%\n",
      "Epoch 76, train loss: 3.390E-01, val loss: 3.303E-01, top1 acc: 88.80%\n",
      "Epoch 77, train loss: 3.363E-01, val loss: 3.372E-01, top1 acc: 88.62%\n",
      "Epoch 78, train loss: 3.407E-01, val loss: 3.342E-01, top1 acc: 88.83%\n",
      "Epoch 79, train loss: 3.387E-01, val loss: 3.314E-01, top1 acc: 88.85%\n",
      "Epoch 80, train loss: 3.363E-01, val loss: 3.363E-01, top1 acc: 88.69%\n",
      "Epoch 81, train loss: 3.378E-01, val loss: 3.300E-01, top1 acc: 88.84%\n",
      "Epoch 82, train loss: 3.372E-01, val loss: 3.376E-01, top1 acc: 88.67%\n",
      "Epoch 83, train loss: 3.364E-01, val loss: 3.336E-01, top1 acc: 88.69%\n",
      "Epoch 84, train loss: 3.385E-01, val loss: 3.365E-01, top1 acc: 88.65%\n",
      "Epoch 85, train loss: 3.390E-01, val loss: 3.366E-01, top1 acc: 88.66%\n",
      "Epoch 86, train loss: 3.340E-01, val loss: 3.344E-01, top1 acc: 88.74%\n",
      "Epoch 87, train loss: 3.379E-01, val loss: 3.418E-01, top1 acc: 88.39%\n",
      "Epoch 88, train loss: 3.372E-01, val loss: 3.354E-01, top1 acc: 88.72%\n",
      "Epoch 89, train loss: 3.363E-01, val loss: 3.331E-01, top1 acc: 88.71%\n",
      "Epoch 90, train loss: 3.385E-01, val loss: 3.338E-01, top1 acc: 88.76%\n",
      "Epoch 91, train loss: 3.355E-01, val loss: 3.340E-01, top1 acc: 88.65%\n",
      "Epoch 92, train loss: 3.360E-01, val loss: 3.341E-01, top1 acc: 88.70%\n",
      "Epoch 93, train loss: 3.325E-01, val loss: 3.331E-01, top1 acc: 88.61%\n",
      "Epoch 94, train loss: 3.382E-01, val loss: 3.352E-01, top1 acc: 88.69%\n",
      "Epoch 95, train loss: 3.369E-01, val loss: 3.369E-01, top1 acc: 88.60%\n",
      "Epoch 96, train loss: 3.376E-01, val loss: 3.330E-01, top1 acc: 88.76%\n",
      "Epoch 97, train loss: 3.377E-01, val loss: 3.381E-01, top1 acc: 88.59%\n",
      "Epoch 98, train loss: 3.357E-01, val loss: 3.364E-01, top1 acc: 88.75%\n",
      "Epoch 99, train loss: 3.426E-01, val loss: 3.359E-01, top1 acc: 88.58%\n",
      "Epoch 100, train loss: 3.397E-01, val loss: 3.362E-01, top1 acc: 88.70%\n"
     ]
    }
   ],
   "source": [
    "for e in range(100):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Send inputs through model\n",
    "        feat = model(x)\n",
    "        y_logit = linear_head(feat).squeeze(1)\n",
    "        loss = F.cross_entropy(y_logit, y)\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        # Backpropagate loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    num_top1_correct = 0\n",
    "    num_top5_correct = 0\n",
    "    total = 0\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_dataloader):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            feat = model(x)\n",
    "            y_logit = linear_head(feat).squeeze(1)\n",
    "            loss = F.cross_entropy(y_logit, y)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            batch_top1, batch_top5 = num_correct(y_logit, y, topk=(1, 5))\n",
    "            num_top1_correct += batch_top1.item()\n",
    "            num_top5_correct += batch_top5.item()\n",
    "            total += len(x)\n",
    "\n",
    "    num_top1_acc = num_top1_correct / total\n",
    "    num_top5_acc = num_top5_correct / total\n",
    "    print(f\"Epoch {e+1}, train loss: {np.mean(epoch_loss):.3E}, val loss: {np.mean(val_loss):.3E}, top1 acc: {num_top1_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('simclr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "733f93a6f46c617944ed1f371f6b9572207877d31130eaa34eb0156c9001b40f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
