{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate needed for notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = \"../results/TransOpFISTA_09-22-2022_20-06-15/\"\n",
    "current_checkpoint = 5\n",
    "device_idx = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kion/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/kion/anaconda3/envs/simclr/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kion/anaconda3/envs/simclr/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import math\n",
    "sys.path.append(os.path.dirname(os.getcwd()) + \"/src/\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import omegaconf\n",
    "\n",
    "from eval.utils import encode_features\n",
    "from model.model import Model\n",
    "from model.config import ModelConfig\n",
    "from experiment import ExperimentConfig\n",
    "from dataloader.contrastive_dataloader import get_dataloader\n",
    "from dataloader.utils import get_unaugmented_dataloader\n",
    "\n",
    "# Set the default device\n",
    "default_device = torch.device(\"cuda:0\")\n",
    "# Load config\n",
    "cfg = omegaconf.OmegaConf.load(run_dir + \".hydra/config.yaml\")\n",
    "cfg.model_cfg.backbone_cfg.load_backbone = None\n",
    "# Load model\n",
    "default_model_cfg = ModelConfig()\n",
    "model = Model.initialize_model(cfg.model_cfg, device_idx)\n",
    "state_dict = torch.load(run_dir + f\"checkpoints/checkpoint_epoch{current_checkpoint}.pt\")\n",
    "model.load_state_dict(state_dict['model_state'])\n",
    "# Manually override directory for dataloaders\n",
    "cfg.train_dataloader_cfg.dataset_cfg.dataset_dir = \"../datasets\"\n",
    "cfg.train_dataloader_cfg.batch_size = 32\n",
    "cfg.eval_dataloader_cfg.dataset_cfg.dataset_dir = \"../datasets\"\n",
    "# Load dataloaders\n",
    "train_dataset, train_dataloader = get_dataloader(cfg.train_dataloader_cfg)\n",
    "eval_dataset, eval_dataloader = get_dataloader(cfg.eval_dataloader_cfg)\n",
    "unaugmented_train_dataloader = get_unaugmented_dataloader(train_dataloader)\n",
    "# Get encoding of entire dataset\n",
    "train_eval_input = encode_features(model, unaugmented_train_dataloader, default_device)\n",
    "# Load transport operators\n",
    "psi = model.contrastive_header.transop_header.transop.get_psi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unaugmented_train_dataloader.dataset.dataset.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 512])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "transform = lambda x: F.adjust_brightness(x, 0.2)\n",
    "\n",
    "coeff_list = []\n",
    "z0_list = []\n",
    "z1_list = []\n",
    "\n",
    "y_list = []\n",
    "\n",
    "for idx, batch in enumerate(unaugmented_train_dataloader):\n",
    "    x, y, idx = batch\n",
    "\n",
    "    x_tilde = [transform(Image.fromarray(single_x.permute(1, 2, 0).detach().numpy().astype(np.uint8))) for single_x in x]\n",
    "    x1 = torch.stack([F.to_tensor(single_x) for single_x in x_tilde])\n",
    "\n",
    "    z0 = model.backbone.backbone_network(x.to(default_device))\n",
    "    z1 = model.backbone.backbone_network(x1.to(default_device))\n",
    "    z0_list.append(z0.detach().cpu())\n",
    "    z1_list.append(z1.detach().cpu())\n",
    "    y_list.append(y)\n",
    "\n",
    "z0_list = torch.concat(z0_list)\n",
    "z1_list = torch.concat(z1_list)\n",
    "y_list = torch.concat(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kion/anaconda3/envs/simclr/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: An output with one or more elements was resized since it had shape [6, 1024, 1024], which does not match the required output shape [1, 6, 1024, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/kion/manifold-contrastive/src/model/manifold/transop.py:18: UserWarning: An output with one or more elements was resized since it had shape [13, 512, 512], which does not match the required output shape [1, 13, 512, 512]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out = torch.matrix_exp(T) @ x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8133301734924316\n",
      "tensor(5.9667, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kion/manifold-contrastive/src/model/manifold/l1_inference.py:13: UserWarning: An output with one or more elements was resized since it had shape [12, 512, 512], which does not match the required output shape [1, 12, 512, 512]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  x1_hat = (torch.matrix_exp(T) @ x0.unsqueeze(-1)).squeeze(-1)\n",
      "/home/kion/manifold-contrastive/src/model/manifold/l1_inference.py:13: UserWarning: An output with one or more elements was resized since it had shape [6, 512, 512], which does not match the required output shape [1, 6, 512, 512]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  x1_hat = (torch.matrix_exp(T) @ x0.unsqueeze(-1)).squeeze(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.41575288772583\n",
      "tensor(8.7667, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_909955/1551082283.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdevnull\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredirect_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 _, c = infer_coefficients(\n\u001b[0m\u001b[1;32m     26\u001b[0m                     \u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/manifold-contrastive/src/model/manifold/l1_inference.py\u001b[0m in \u001b[0;36minfer_coefficients\u001b[0;34m(x0, x1, psi, zeta, max_iter, tol, num_trials, device, c_init)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mc_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchange_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mc_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mopt_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/simclr/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/simclr/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from model.manifold.transop import TransOp_expm\n",
    "from model.manifold.l1_inference import infer_coefficients\n",
    "import os, contextlib\n",
    "\n",
    "class_to_use = 8\n",
    "zeta = 0.2\n",
    "\n",
    "class_idx = y_list == class_to_use\n",
    "z0_use = z0_list[class_idx]\n",
    "z1_use = z1_list[class_idx]\n",
    "z0_train, z1_train = z0_use[:int(len(z0_use)*.8)], z1_use[:int(len(z1_use)*.8)]\n",
    "z0_test, z1_test = z0_use[int(len(z0_use)*.8):], z1_use[int(len(z1_use)*.8):]\n",
    "\n",
    "transop = TransOp_expm(10, 512).to(default_device)\n",
    "opt = torch.optim.SGD(transop.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "opt_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.99)\n",
    "\n",
    "for i in range(100):\n",
    "    train_error = []\n",
    "    for j in range(len(z0_train) // 30):\n",
    "        z0, z1 = z0_train[j*30:(j+1)*30].to(default_device), z1_train[j*30:(j+1)*30].to(default_device)\n",
    "        with open(os.devnull, 'w') as devnull:\n",
    "            with contextlib.redirect_stdout(devnull):\n",
    "                _, c = infer_coefficients(\n",
    "                    z0,\n",
    "                    z1,\n",
    "                    transop.get_psi(),\n",
    "                    zeta,\n",
    "                    max_iter=500,\n",
    "                    num_trials=1,\n",
    "                    device=default_device,\n",
    "                )\n",
    "        with open(os.devnull, 'w') as devnull:\n",
    "            with contextlib.redirect_stdout(devnull):\n",
    "                z1_hat = transop(z0.unsqueeze(-1), c).squeeze(dim=-1)\n",
    "        \n",
    "        loss = F.mse_loss(z1_hat, z1)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt_scheduler.step()\n",
    "        train_error.append(loss.item())\n",
    "        break\n",
    "    print(np.mean(train_error))\n",
    "    print(c.count_nonzero(dim=-1).float().mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kion/anaconda3/envs/simclr/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kion/anaconda3/envs/simclr/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 512])\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms.transforms as T\n",
    "import torch\n",
    "\n",
    "cifar10 = torchvision.datasets.CIFAR10(\n",
    "    \"../datasets\",\n",
    "    train=True,\n",
    "    transform=T.Compose(\n",
    "        [\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(\n",
    "                mean=[0.50707516, 0.48654887, 0.44091784],\n",
    "                std=[0.26733429, 0.25643846, 0.27615047],\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    cifar10, batch_size=100, shuffle=False, num_workers=20\n",
    ")\n",
    "\n",
    "backbone = models.resnet18(pretrained=True).to('cuda:0')\n",
    "backbone.fc = torch.nn.Identity()\n",
    "\n",
    "z_list = []\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    x, y = batch\n",
    "    x = x.to('cuda:0')\n",
    "    z = backbone(x)\n",
    "\n",
    "    z_list.append(z.detach().cpu())\n",
    "z = torch.concat(z_list)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "simm = pairwise_distances(z, z)\n",
    "sort_sim = np.argsort(simm, axis=-1)[:,1:6]\n",
    "print(sort_sim.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('simclr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "733f93a6f46c617944ed1f371f6b9572207877d31130eaa34eb0156c9001b40f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
